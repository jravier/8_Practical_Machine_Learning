---
title: "Practical Machine Learning"
author: "jravier"
date: "22/04/2020"
output: 
  bookdown::html_document2: 
    df_print: kable
    number_sections: yes
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

# Overview  
Data was collected using body sensors on people performing a physical activity. The goal is to quantify how well they do it.  
After careful exploratory data analysis, we found that the outcomes can be predicted straight away from the meta data provided.  
We also show how a more realistic approach could be used to predict these outcomes from the sensor output, using a model trained on the provided data and cross validated.  
The final model is tested against 20 case with unknown outcomes.  

# Exploratory data analysis and strategy  
Two data set are provided (one for training and one with the final 20 test cases). We will use R to analyses them and build our models. Here are the R packages used:
```{r dependancies, include=FALSE, cache=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(caret)
library(parallel)
library(doParallel)
```

## Basic data cleaning  
Loading the training data set somewhere (in R, in a text editor or in Excel), we see plenty of rows, plenty of columns, plenty of NAs, one date variable and some factors variables, so let's load the data properly.  
The most important points are :  

* correct handling of NAs
* correct levels for the factor variables  

```{r load, warning=FALSE}
pml_training <- read_csv("pml-training.csv", na=c("NA","#DIV/0!"), col_types = cols(
    X1 = col_skip(), 
    user_name = col_factor(levels = c("adelmo", "carlitos", "charles", "eurico", "jeremy", "pedro")), 
    cvtd_timestamp = col_datetime(format = "%d/%m/%Y %H:%M"), 
    new_window = col_factor(levels = c("yes", "no")), 
    num_window = col_integer(),
    classe = col_factor(levels = c("A", "B", "C", "D", "E"))))

pml_testing <- read_csv("pml-testing.csv", na=c("NA","#DIV/0!"), col_types = cols(
    X1 = col_skip(), 
    user_name = col_factor(levels = c("adelmo", "carlitos", "charles", "eurico", "jeremy", "pedro")), 
    cvtd_timestamp = col_datetime(format = "%d/%m/%Y %H:%M"), 
    new_window = col_factor(levels = c("yes", "no")), 
    num_window = col_integer()))
```

Given what is said in the course project instructions and the experiment description page at http://groupware.les.inf.puc-rio.br/har, we infer that:

* The last column is the outcome.
* All the columns with belt, forearm, arm, and dumbbell in their names are measurements.
* The first columns (up to 'num_window') are meta data about the measurements.

## Parsimonious strategy  

The course project instructions states that "You may use any of the other variables to predict with". So, can we use the meta data to predict the outcome ?

* The observations are organized in time series (time stamps are increasing).
* The observations are organized into windows (as explained in the article at http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf, linked from the experiment description page):
    1. The article explain that each window correspond to a single subject (user), and most importantly to A SINGLE OUTCOME.
    2. We see that windows are numbered and that there are window numbers both in the training and the testing data sets, which means that we could use the test set window number to predict the outcome by looking for it in the training set.
    3. But our test (20 observations) and training (~19 000 obs) sets are just subsets of the original data (~39 000 obs), so there is a possibility that some test set window numbers may not exist in the train set.

Let's confirm the first point:
```{r win check}
winCheck<-pml_training %>%
    group_by(num_window) %>%
    summarise(num_classe=n_distinct(classe), num_user=n_distinct(user_name))

summary(winCheck)
```

So this is the case: window numbers are not repeated among users and only ONE outcome is possible for each window number.  


**Hence, if (and only if) all the window numbers in the test set are present in the training set, we can predict, with 100% accuracy and without any training, the outcome from the window number variable.**  

## More data cleaning
If we can't use our simple strategy (non matching window numbers), we'll have to resort to training a model on the measurements and predict the outcome from this model. In this case, can we reduce computing resources by removing some variables?

* We see a lot of columns with mostly NAs. 
    * All the NA values are for observations with the variable 'new_window' set to 'no', while the non NA values are for 'yes'. 
    * All these columns have a statistic calculus in their names (min, avg, kurtosis, var, etc.). This means that these variables contains only statistics about the whole windows, not about individual observations.
* None of the 20 observations in the test set are new windows and they have all NA values in these statistics variables. Test set variables are either full of NA or have no NA at all:

```{r chack NA}
levels(as.factor(sapply(pml_testing, function(x) sum(is.na(x)))))
```

So we won't be able to predict anything with these variables full of NA and we can safely remove all these columns from both the training and testing sets:  

```{r remove NA}
useless<-sapply(pml_testing, function(x) sum(is.na(x)))==20
pml_testing <- pml_testing[,useless==FALSE]
pml_training<-pml_training[,useless==FALSE]
```

We can also check if some training variables have too little variation to be useful for training:  
```{r little var}
nearZeroVar(pml_training[,7:58])
```

This is not the case and we keep all the remaining columns.  

# Easy answer: 100% accuracy prediction without training
Using a left join (test set on the left), let's try to predict the outcome for each observation in the test set. If we have any NA coming from the right side (variables from training set), the window numbers are not fully matching and prediction can't be made for those unmatching window numbers:  
```{r predict win num}
winFit<-pml_training %>%
    group_by(num_window, classe) %>%
    summarise(lines=n())

win.test.predict <- pml_testing %>%
    select(num_window, problem_id) %>%
    left_join(winFit, by="num_window")

win.test.predict$classe
```

No NA... The prediction is complete!  
More importantly, it's done with 100% accuracy, without any model, no need for cross validation and 0% out of sample error.  
  
This approach is the most **parsimonious** (given the available variables and the fact that we are allowed to use anyone of them). All it needs is to _read the documentation_!
  
Please note that even if the window numbers were missing or not matching, we could try the same approach by using the user name and time stamps variables: since all observations are taken in time series and each time series correspond to a single outcome, it's possible to match the values in the test set to values in the training set:  
```{r timePlot, cache=FALSE, fig.align="right", fig.height=3, fig.cap="The activity pattern is similar for each participant."}
timeboundaries <- pml_training %>%
    group_by(user_name) %>%
    summarise(start=min(raw_timestamp_part_1), 
              end = max(raw_timestamp_part_1)+1)

timedata <- pml_training %>%
    inner_join(timeboundaries, by="user_name") %>%
    transmute(user_name, 
              time = raw_timestamp_part_1 - start + raw_timestamp_part_2 / 1000000,
              timefrac = time/(end - start), 
              classe)

par(mar=c(4.1, 4.5, 4.1, 5.1), xpd=TRUE)
plot(timedata$timefrac, timedata$user_name, col=timedata$classe, 
     yaxt = "n", ylab="", xlab="relative fractional time", 
     main="normalized time lines of the observations")
mtext("participant", 2, line=3.6)
axis(2, labels = levels(timedata$user_name), 
     at=1:length(levels(timedata$user_name)), las = 2)
legend("right", inset = c(-0.2,0), title = "classe", pch = 1, 
       levels(timedata$classe), col=1:length(levels(timedata$classe)))
```
  
    
  
  
_Okay, I'll stop here: you got my point and this is a course about machine learning, so I'm supposed to show how I can train a model, evaluate it and predict the outcomes from it... _ 


# More realistic answer: prediction with only the physical measurements  
_Disclaimer: Yes, this approach is more realistic, as in real time application of such a device, there aren't many meta data. But please note that this is not what would be done in the real world: the authors of the experiment explain in their article (http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) that individual observations are too noisy and only the statistic variables on full windows are used for prediction._  
However, as the test set observations aren't full window observations (they don't have any value for these statistics), we'll have to make predictions directly from the remaining variables: sensors outputs (or combinations of them) in each observation.  
  
## Model choice  
The authors of the study state that "because  of  the  characteristic  noise  in  the  sensor  data,  we used  a  Random  Forest  approach" and cite this reference to justify it: "L. B. Statistics and L. Breiman. Random forests. In _Machine Learning_, pages 5â€“32, 2001".

As we don't have any idea of what is this "characteristic  noise", let's look at the distributions of the variables (notice that this allow us to spot 2 observations with BIG outliers):
```{r noise, fig.align="right", fig.height=25, fig.cap="The distributions of the values of the variables doesn't show a clear signal."}
#From the training set:
#   - remove 2 observations with BIG outliers (so plots aren't crushed)
#   - Put it in the long form
pml_long<-pml_training[,7:59] %>%
    filter_at(vars(contains("gyros_")), all_vars(abs(.)<10)) %>%
    filter_at(vars(contains("magnet_")), all_vars(abs(.)<2000)) %>%
    gather("variable", "value", 1:52)

# Add some identifiers for facets' cols & rows 
#   and boxplot each variable against the Outcome
pml_long %>%
    mutate(sensor = sub("(.+?)_(arm|belt|dumbbell|forearm)(:?$|(_.+?))","\\2",variable),
           measure= sub("(.+?)_(arm|belt|dumbbell|forearm)(:?$|(_.+?))","\\1\\3",variable)) %>%
    ggplot(., aes(classe, value)) +
    geom_boxplot() +
    facet_grid(cols = vars(sensor), rows = vars(measure), scales = "free_y") +
    labs(title = "Variable distributions in the training data set for each outcome,\n discriminated by sensor location and measurement taken.")
```
For many variables, most of the values are in the same range for all the outcomes, so indeed, the outcome signal is burried into noise...  
So we will also train a Random Forest Model in order to get through it.  
Only if this approach fails to give a good enough accuracy, we will have to either tune further the model or try some other models.  

## Model building  
Here are the preliminary steps:

* Keep only the physical measurements variables.
* Separate the training set into an actual training set and a validation set. This will serve 2 goals:
    * Reduce the size of the training set to improve the model computing time.
    * Provide another, independent way to evaluate the out-of-sample error (without having to use the test set). We will use cross validation, which should be enough to evaluate the out-of-sample error from the resample, but having another validation set allow for comparison.
* Configure trainControl object for cross validation (using method 'cv' and default fold number of 10).

```{r RF preparation}
# Create training & validation data sets with only physical measurements variables
inTraining <- createDataPartition(pml_training$classe, p = .75)[[1]]
training <- pml_training[inTraining,7:59]
validation <- pml_training[-inTraining,7:59]

# Configure trainControl object
fitControl <- trainControl(method = "cv",
                           allowParallel = TRUE)
```
  
Then the big part:

 * Configure parallel processing, following instructions linked from the forum : (https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) 
 * Train the model (the long part!)
 * And finally stop the parallel processing...
 
```{r RF Model, cache=TRUE}
# Configure parallel processing
cluster <- makeCluster(detectCores()-1) # convention to leave 1 core for OS
registerDoParallel(cluster)

# Run training
TimeToFit<-system.time(rfFit <- train(classe ~ .,
                                      method="rf", 
                                      trControl = fitControl,
                                      data=training, 
                                      ntree=50))

# De-register parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
    
    TimeToFit
```

Even with 50 trees (instead of default 500) it took a while, so let's first check if the number of trees was enough to converge:
```{r RFCheck, fig.align="right", fig.height=7, fig.cap="Convergence of the model is outlined by the decrease in error rate as the number of tress increases.", cache=FALSE}
matplot(rfFit$finalModel$err.rate, type = 'l', 
        xlab = "trees",
        ylab = "error rate",
        main = "Error by number of trees in the RF model")
abline(h = 0.01)
legend("topright", colnames(rfFit$finalModel$err.rate), lty=1:length(colnames(rfFit$finalModel$err.rate)), col=1:length(colnames(rfFit$finalModel$err.rate)))
```

The model converged well, giving a final Out-Of-Bag (OOB) estimate of error rate of less than 1%.  
The chosen number of tree is enough to achieve good accuracy and avoid both overfitting and wasting computer resources.

## Model Details  
Now we can look closer at the model:  
```{r RF Details}
rfFit
```

```{r RF conf train}
confusionMatrix.train(rfFit)
```

Accuracy is very good, so let's evaluate the out of sample error.  

## Cross validation  
Out of sample error can be evaluated in 2 ways:

* using the resamples inside the model (remember we used a 10 fold 'cv' train control).
* using the validation set that we put apart before building the model.  

So, first, we find the accuracy given by a prediction on the validation set:  
```{r RF Validation}
rfValidPred <- predict(rfFit, newdata = validation)
rfValidCM<-confusionMatrix(rfValidPred, validation$classe)
rfValidCM
```

Accuracy is fine again on the validation data.  
We can now compare the 2 out-of-sample errors:
```{r RF oos errors}
round(rbind("10 fold resample"=c("out-of-sample error"=1-mean(rfFit$resample$Accuracy)),
            "validation set"=1-rfValidCM$overall[['Accuracy']]),4)
```
Out of sample errors estimated from the resample or from the prediction on the validation data set are similar (and very low), giving us confidence in the abality of our model to predict well any new observation.  

Estimate on the resample is larger because it is an average on ALL the resample, meaning on all the actual training set which is 3 times bigger (75% of the original training set) than the validation set (25%).

## Test set prediction  
Given that the expected accuracy of the model is `r round(min(mean(rfFit$resample$Accuracy), rfValidCM$overall[['Accuracy']]), 4)` (let's take the worst estimate), and assuming that the 20 test set's observations are independent, our chance to predict all them right is:
$$`r round(min(mean(rfFit$resample$Accuracy), rfValidCM$overall[['Accuracy']]), 4)`^{20} = `r round(100*min(mean(rfFit$resample$Accuracy), rfValidCM$overall[['Accuracy']])^20,0)` \%$$  

This seems enough, we don't need to tune further the model or try another one.  
So let's dor our final prediction of the outcomes on the test set:
```{r RF test}
predict(rfFit, newdata = pml_testing)
```
These predicted values are the same as the one obtained by the deterministic method.  
  
# Conclusion  
Even though the outcomes could be predicted with 100% accuracy using the window number variable, we have shown that a Random Forest model (chosen because of its robustness against noise), constructed with only 50 trees and using a 10-fold cross-validation, could predict the outcomes from the physical measurement variables with a very low expected out of sample error, enough to have the 20 test observation predicted right.
